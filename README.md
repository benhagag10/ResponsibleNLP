# Fairness and Responsible NLP Resources

This repository is a collection of papers, datasets, and resources related to fairness, transparency, and ethical considerations in Natural Language Processing (NLP). I am using it for my ownn research but i believe other community members might see this respository usefull as well. 

This repository is open to contributions!

## ðŸ“š **Papers**  

- **"Bias and Fairness in Large Language Models: A Survey"** â€“ Link: [https://aclanthology.org/2024.cl-3.8.pdf](https://aclanthology.org/2024.cl-3.8.pdf)  
- **"Quantifying the Persona Effect in LLM Simulations"** â€“ Link: [https://aclanthology.org/2024.acl-long.554.pdf](https://aclanthology.org/2024.acl-long.554.pdf)


## ðŸ“Š **Datasets**  

- **"Bias in Bios" Dataset** â€“ A dataset used for studying bias in predicting occupations from biographies.  
Link: [https://github.com/MitchellResearch/BiasInBios](https://github.com/MitchellResearch/BiasInBios)

- **"CrowS-Pairs" Dataset** â€“ A benchmark dataset for measuring bias in masked language models.  
Link: [https://github.com/nyu-mll/crows-pairs](https://github.com/nyu-mll/crows-pairs)

- **"HolisticBias" Dataset** â€“ A large-scale dataset for evaluating bias in text generation.  
Link: [https://github.com/holistic-bias/holistic-bias](https://github.com/holistic-bias/holistic-bias)


## ðŸ›  **Resources & Tools**  

- **"FairSeq"** â€“ A toolkit for training and evaluating models with fairness constraints.  
Link: [https://github.com/facebookresearch/fairseq](https://github.com/facebookresearch/fairseq)

- **"The AI Fairness 360 Toolkit"** â€“ A Python toolkit to help detect and mitigate bias in machine learning models.  
Link: [https://github.com/IBM/AIF360](https://github.com/IBM/AIF360)

- **"Fairness Indicators"** â€“ A TensorFlow tool for evaluating fairness metrics.  
Link: [https://github.com/tensorflow/fairness-indicators](https://github.com/tensorflow/fairness-indicators)
